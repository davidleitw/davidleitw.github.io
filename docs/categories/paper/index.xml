<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>paper - Category - davidLei</title>
        <link>https://davidleitw.github.io/categories/paper/</link>
        <description>paper - Category - davidLei</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>davidleitw@gmail.com (davidlei)</managingEditor>
            <webMaster>davidleitw@gmail.com (davidlei)</webMaster><lastBuildDate>Mon, 19 Dec 2022 22:13:53 &#43;0800</lastBuildDate><atom:link href="https://davidleitw.github.io/categories/paper/" rel="self" type="application/rss+xml" /><item>
    <title>Bypassing the Load Balancer Without Regrets - SoCC ’20</title>
    <link>https://davidleitw.github.io/posts/paper01/</link>
    <pubDate>Mon, 19 Dec 2022 22:13:53 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://davidleitw.github.io/posts/paper01/</guid>
    <description><![CDATA[前言 最近為了找論文題目看了不少論文，剛好趁這個機會寫點文章當作讀這些 paper 的學習筆記，這篇 主要提出一個新的 Load balance 的機制，希望可以消除 LB 在 data path 花費的時間，讓 LB 專注於處理連線請求，決定 connection 去往哪台 server 之後就讓 Client 與 server 之間直接連線，細節就看底下的簡介或者論文內文吧，我覺得這篇 paper 在概念講解的地方畫了幾張不錯的示意圖，如果想要解釋 L4 的 load balancer 怎麼運作的，這是一篇滿推薦的論文。
這篇論文的作者也用了 P4, eBPF 等技術實現了他們的論文，如果有興趣的也可以研究看看實現的 code
常見 LB 技術分類 這篇論文在前面的幾個段落介紹了常見了 Load Belance 技術，然後規劃了一些簡單的實驗來分析各種不同技術的優劣，文中用以下五個標準/指標來分類不同種類的 LB 技術，如下圖所示
比較圖表如下:
其中 L4 的 Load Balancer 以形式可以分成有沒有支援 DSR(Direct Server Return)，又或者分為 Stateless 和 Stateful，以筆者的理解大規模的服務來說會盡量把 Load Balancer 設計成無狀態的，這樣才能把雲端環境的動態優勢發揮出來，像是另一篇關於 LB 的論文 A High-Speed Load-Balancer Design with Guaranteed Per-Connection-Consistency NSDI2020 有提到一個觀點，為什麼現在主流的 LB 技術都不依靠像是 weighted round robin 或者 power of two choices 這種可以根據伺服器狀態來最佳化負載的演算法，以下引用該文章的說法]]></description>
</item><item>
    <title>The Google File System (GFS) 論文心得</title>
    <link>https://davidleitw.github.io/posts/gfs/</link>
    <pubDate>Fri, 13 Aug 2021 21:34:43 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://davidleitw.github.io/posts/gfs/</guid>
    <description><![CDATA[跟著 MIT6.824 的課程進度， 在 Lec3 終於拜讀了這篇經典的論文，GFS 是一個分散式的檔案系統，由 Google 開發。 在2003年時 Google 發表了 The Google File System 這篇論文來分享一些 GFS 開發上的細節， 雖然 Google 沒有開源這個著名的檔案系統， 不過還是可以由 paper 來窺探當初 Google 設計的細節以及整體的架構。
背景 為了滿足當初 Google 日漸增長的資料處理需求， Google開發了 GFS 這套分散式檔案系統。
用分散式的原因: 要處理的檔案大小已經遠遠超過單機可以容納的資料量
跟過往很多分散式系統一樣，GFS 也追求著以下幾點需求:
performance 性能 scalability 擴展性 reliability 可靠性 availability 可用性 因為成本以及其他因素的考量之下，Google 並沒有選擇商業用的 server 來製作 GFS 這套系統， 而且採用一般的 普通主機(inexpensive commidity hardware) 來作為叢集成員。因為是普通的主機，所以沒有商業用的 server 穩定， 在設計 GFS 的時候必須一併考慮容錯問題來增加穩定性。
在 Introduction 中也有先簡單介紹了 GFS 的幾個特色
元件失效(component failures) 被認為是常態 GFS 中包含著數以百計或者更多的儲存機器，每一台都有可能在任何一個時間點發生不可預期的錯誤，也有些錯誤是無法從中恢復的。 所以 GFS 需要實現持續的監控，錯誤偵測，容錯跟自動恢復等功能。]]></description>
</item></channel>
</rss>
